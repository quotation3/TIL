{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNe2nigiT9qsDCJ7T9LMHCT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quotation3/TIL/blob/master/LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1DaaLv3c1Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_ls = [\"Cute kitty\",\n",
        "          \"Eat rice or cake\",\n",
        "          \"Kitty and hamster\",\n",
        "          \"Eat bread\",\n",
        "          \"Rice , bread and cake\",\n",
        "          \"Cute hamster eats bread and cake\"]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS3j1Z5Ec470",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "4ae082cf-2a98-469c-d6bf-689bc2dc4983"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNw2GHCwc7NC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "9f4a630c-dc86-4a4c-d1f7-cca06aa71fb8"
      },
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# 문장 전처리\n",
        "pos_docs = []\n",
        "for line in docs_ls:\n",
        "    doc = line.split(\" \")\n",
        "    tmp_docs = []\n",
        "    for word in doc:\n",
        "        # 소문자화, Lemmatize\n",
        "        tmp_docs.append(wl.lemmatize(word.lower(), pos = 'v' or 'n'))\n",
        "    # 영어 품사 부착(PoS Tagging)\n",
        "    pos_docs.append(pos_tag(tmp_docs))\n",
        "\n",
        "pos_docs"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('cute', 'NN'), ('kitty', 'NN')],\n",
              " [('eat', 'NN'), ('rice', 'NN'), ('or', 'CC'), ('cake', 'VB')],\n",
              " [('kitty', 'NNS'), ('and', 'CC'), ('hamster', 'NN')],\n",
              " [('eat', 'NN'), ('bread', 'NN')],\n",
              " [('rice', 'NN'), (',', ','), ('bread', 'NN'), ('and', 'CC'), ('cake', 'NN')],\n",
              " [('cute', 'NN'),\n",
              "  ('hamster', 'NN'),\n",
              "  ('eat', 'NN'),\n",
              "  ('bread', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('cake', 'NN')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xRJnyCIc9_f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "a0f564d8-c339-430e-e19c-79d191212981"
      },
      "source": [
        "stopPos = ['CC']\n",
        "stopWord = [',']\n",
        "\n",
        "docs_token = []\n",
        "tokens = []\n",
        "\n",
        "for pos_doc in pos_docs:\n",
        "    doc_token_tmp = []\n",
        "    for pos_token in pos_doc:\n",
        "        # 불용 품사 지정\n",
        "        if pos_token[1] not in stopPos:\n",
        "            # 불용어 지정\n",
        "            if pos_token[0] not in stopWord:\n",
        "                doc_token_tmp.append(pos_token[0])\n",
        "                tokens.append(pos_token[0])\n",
        "    # 문서 사용 단어\n",
        "    docs_token.append(doc_token_tmp)\n",
        "# 전체 문서 단어\n",
        "token_list = list(set(tokens))\n",
        "\n",
        "docs_token, tokens, token_list"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['cute', 'kitty'],\n",
              "  ['eat', 'rice', 'cake'],\n",
              "  ['kitty', 'hamster'],\n",
              "  ['eat', 'bread'],\n",
              "  ['rice', 'bread', 'cake'],\n",
              "  ['cute', 'hamster', 'eat', 'bread', 'cake']],\n",
              " ['cute',\n",
              "  'kitty',\n",
              "  'eat',\n",
              "  'rice',\n",
              "  'cake',\n",
              "  'kitty',\n",
              "  'hamster',\n",
              "  'eat',\n",
              "  'bread',\n",
              "  'rice',\n",
              "  'bread',\n",
              "  'cake',\n",
              "  'cute',\n",
              "  'hamster',\n",
              "  'eat',\n",
              "  'bread',\n",
              "  'cake'],\n",
              " ['bread', 'rice', 'kitty', 'cute', 'cake', 'hamster', 'eat'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAGHJ5MCgYzj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9a4605a3-7856-47d9-b892-9c312c7fd2ad"
      },
      "source": [
        "from random import randint\n",
        "\n",
        "# 랜덤으로 토픽부여\n",
        "topic_num = 2\n",
        "topic_list = []\n",
        "\n",
        "for i in range(len(docs_token)):\n",
        "    topic_list.append([randint(1, topic_num) for a in range(len(docs_token[i]))])\n",
        "topic_list"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1], [2, 2, 1], [2, 2], [2, 2], [1, 1, 2], [1, 1, 2, 2, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iruFXLKwhEd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3fc79f0-c451-4a9a-bd8b-c70d1c622adb"
      },
      "source": [
        "# 문서 내 토픽분포\n",
        "topic_cnt = []\n",
        "alpha = 0.1\n",
        "for i in range(1,topic_num+1):\n",
        "    tmp = []\n",
        "    for j in range(len(topic_list)):\n",
        "        tmp.append(topic_list[j].count(i)+alpha)\n",
        "    topic_cnt.append(tmp)\n",
        "topic_cnt"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.1, 1.1, 0.1, 0.1, 2.1, 3.1], [1.1, 2.1, 2.1, 2.1, 1.1, 2.1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWrlWM58nObN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "b76604f9-88f9-46e7-a77e-d840e58d82b8"
      },
      "source": [
        "# 토픽 내 단어분포\n",
        "topic_word = [[0 for a in range((len(tokens)))] for b in range(topic_num)]\n",
        "\n",
        "for i in range(len(docs_token)):\n",
        "    for j in range(len(docs_token[i])):\n",
        "        for k in range(1, topic_num+1):\n",
        "            if topic_list[i][j] == k:\n",
        "                    topic_word[k-1][tokens.index(docs_token[i][j])] += 1\n",
        "\n",
        "beta = 0.001\n",
        "for i in range(len(topic_word)):\n",
        "    for j in range(len(topic_word[i])):\n",
        "        topic_word[i][j] += beta\n",
        "\n",
        "topic_word"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.001,\n",
              "  1.001,\n",
              "  0.001,\n",
              "  1.001,\n",
              "  2.001,\n",
              "  0.001,\n",
              "  1.001,\n",
              "  0.001,\n",
              "  1.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001],\n",
              " [1.001,\n",
              "  1.001,\n",
              "  3.001,\n",
              "  1.001,\n",
              "  1.001,\n",
              "  0.001,\n",
              "  1.001,\n",
              "  0.001,\n",
              "  2.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001,\n",
              "  0.001]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DStuDJN6eYP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "50975c3a-f7fb-435f-b239-a1b8ef0a4c85"
      },
      "source": [
        "# 첫 단어의 토픽을 0으로 설정\n",
        "topic_list[0][0] = 0\n",
        "\n",
        "topic_cnt = []\n",
        "alpha = 0.1\n",
        "for i in range(1,topic_num+1):\n",
        "    tmp = []\n",
        "    for j in range(len(topic_list)):\n",
        "        tmp.append(topic_list[j].count(i)+alpha)\n",
        "    topic_cnt.append(tmp)\n",
        "topic_cnt\n",
        "\n",
        "topic_word = [[0 for a in range((len(tokens)))] for b in range(topic_num)]\n",
        "\n",
        "for i in range(len(docs_token)):\n",
        "    for j in range(len(docs_token[i])):\n",
        "        for k in range(1, topic_num+1):\n",
        "            if topic_list[i][j] == k:\n",
        "                    topic_word[k-1][tokens.index(docs_token[i][j])] += 1\n",
        "\n",
        "beta = 0.001\n",
        "for i in range(len(topic_word)):\n",
        "    for j in range(len(topic_word[i])):\n",
        "        topic_word[i][j] += beta\n",
        "\n",
        "topic_cnt, topic_word"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[1.1, 1.1, 0.1, 0.1, 2.1, 3.1], [0.1, 2.1, 2.1, 2.1, 1.1, 2.1]],\n",
              " [[1.001,\n",
              "   1.001,\n",
              "   0.001,\n",
              "   1.001,\n",
              "   2.001,\n",
              "   0.001,\n",
              "   1.001,\n",
              "   0.001,\n",
              "   1.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001],\n",
              "  [0.001,\n",
              "   1.001,\n",
              "   3.001,\n",
              "   1.001,\n",
              "   1.001,\n",
              "   0.001,\n",
              "   1.001,\n",
              "   0.001,\n",
              "   2.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001,\n",
              "   0.001]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8zf1HrchJbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "67fdad2b-201d-4402-e976-b6049fd52ae2"
      },
      "source": [
        "total_dc = 0    # 토픽1이 문서a일 확률의 분모\n",
        "total_tp = 0    # 첫번째 단어가 토픽1일 확률의 분모\n",
        "pw_dc = []  # 토픽1이 문서a일 확률\n",
        "pw_tp = []  # 첫번째 단어가 토픽1일 확률\n",
        "for i in range(len(topic_cnt)):\n",
        "    total_dc += topic_cnt[i][0]\n",
        "for i in range(len(topic_cnt)):\n",
        "    pw_dc.append(topic_cnt[i][0]/total_dc)\n",
        "for i in range(len(topic_word)):\n",
        "    pw_tp.append(topic_word[i][0]/sum(topic_word[i]))\n",
        "\n",
        "pw_final = []   # 첫번째 단어가 문서a 안에 있을때 토픽1일 확률\n",
        "for i in range(len(pw_dc)):\n",
        "    pw_final.append(pw_dc[i]*pw_tp[i])\n",
        "pw_final    # 토픽1일 확률이 더 높으니 토픽1로 설정"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.13076575934634926, 9.241802521163732e-06]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQx-9OaLhWOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5cb3675-063e-4a3d-87eb-e625d39c765c"
      },
      "source": [
        "topic_list[0][0]=pw_final.index(max(pw_final))+1\n",
        "topic_list"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1], [2, 2, 1], [2, 2], [2, 2], [1, 1, 2], [1, 1, 2, 2, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bub7C-WnIYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모든 단어를 순서대로 0으로 설정후 반복"
      ],
      "execution_count": 81,
      "outputs": []
    }
  ]
}